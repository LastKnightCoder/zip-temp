{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e43188-aae3-41f3-a49f-434d8c2caec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 26 13:20:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 44%   40C    P8    28W / 350W |      0MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0f0b5a-a640-437e-b46e-915243b0bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import cv2 as cv\n",
    "import os.path as path\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb8142c-8147-4b10-ac31-5cacbb0a59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'wgan-content-loss'\n",
    "model_dir = 'wgan-content-loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25efbd1b-3172-45cc-8491-41d97a06c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, origin_root, train_root):\n",
    "        super().__init__()\n",
    "        self.origin_root = origin_root\n",
    "        self.train_root = train_root\n",
    "        self.origin_files = os.listdir(self.origin_root)\n",
    "        self.train_files = os.listdir(self.train_root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.origin_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        origin_data = cv.imread(path.join(self.origin_root, self.origin_files[index]))\n",
    "        train_data = cv.imread(path.join(self.train_root, self.train_files[index]))\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        origin_data = to_tensor(origin_data)[0].reshape((1, 678, 384))\n",
    "        return origin_data, to_tensor(train_data)[0].reshape((1, 678, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da21a6b3-9b09-40e3-b738-bc91387321e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.any(torch.isnan(x)):\n",
    "            torch.isnan(x)\n",
    "        residual = x\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        if residual.size()[1] != out.size()[1]:\n",
    "            residual = self.proj(residual)\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # input is (1) x 678 x 384\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (64) x 678 x 384\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (128) x 339 x 192\n",
    "            ResNetBlock(128, 128),\n",
    "            ResNetBlock(128, 256, stride=2),\n",
    "            ResNetBlock(256, 256),\n",
    "            ResNetBlock(256, 512, stride=2),\n",
    "            # state size. (512) x 85 x 48\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (256) x 170 x 96\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (128) x 340 x 192\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (64) x 680 x 384\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=7, stride=1, padding=(4, 3)),\n",
    "            nn.Tanh()\n",
    "            # output is (1) x 678 x 384\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs) + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a0e2f8-132a-4bb3-a6ba-5f17f6055bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # input is (1) x 678 x 384\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (64) x 178 x 89\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (128) x 89 x 44\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(1024, 1, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15f93ab-0502-4079-a6de-5825d59230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model_name = f'{model_dir}/{model_name}-generator.pth'\n",
    "discriminator_model_name = f'{model_dir}/{model_name}-discriminator.pth'\n",
    "start_point = 0\n",
    "\n",
    "class WGAN(object):\n",
    "    def __init__(self):\n",
    "        super(WGAN, self).__init__()\n",
    "        print(\"WGAN With Content Loss Init Model.\")\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.G = Generator().to(self.device)\n",
    "        self.D = Discriminator().to(self.device)\n",
    "        self.batch_size = 16\n",
    "        self.generator_iters = 3000\n",
    "        self.critic_iter = 5\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "        self.lambda_term = 10\n",
    "        \n",
    "        self.loss_function = nn.L1Loss()\n",
    "\n",
    "        self.one = torch.tensor(1, dtype=torch.float).to(self.device)\n",
    "        self.mone = (self.one * -1).to(self.device)\n",
    "\n",
    "        dataset = ImageDataset(origin_root='dataset/train/images',\n",
    "                               train_root='dataset/train/labels')\n",
    "        self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.data = self.get_infinite_batches()\n",
    "\n",
    "        self.init_model()\n",
    "\n",
    "    def init_model(self):\n",
    "        if os.path.exists(generator_model_name) and os.path.exists(discriminator_model_name):\n",
    "            self.G.load_state_dict(torch.load(generator_model_name))\n",
    "            self.D.load_state_dict(torch.load(discriminator_model_name))\n",
    "            print(f'Models load from {generator_model_name} & {discriminator_model_name}')\n",
    "        else:\n",
    "            print('No trained_models found, init new trained_models')\n",
    "            self.G.apply(self.weights_init)\n",
    "            self.D.apply(self.weights_init)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.generator_iters):\n",
    "            begin = time.time()\n",
    "            print(f'Epoch: {epoch + 1 + start_point} / {self.generator_iters + start_point} ============================')\n",
    "            # 训练判别器\n",
    "            for p in self.D.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            d_loss=0\n",
    "            Wasserstein_D=0\n",
    "            # 训练 5 次判别器，训练 1 次生成器\n",
    "            for d_iter in range(self.critic_iter):\n",
    "                self.D.zero_grad()\n",
    "                inputs, reals = next(self.data)\n",
    "                \n",
    "                if inputs.size()[0] != self.batch_size:\n",
    "                    continue\n",
    "\n",
    "                d_loss_real = self.D(reals).mean()\n",
    "                d_loss_real.backward(self.mone)\n",
    "\n",
    "                fakes = self.G(inputs)\n",
    "                d_loss_fake = self.D(fakes).mean()\n",
    "                d_loss_fake.backward(self.one)\n",
    "\n",
    "                gradient_penalty = self.calculate_gradient_penalty(reals.data, fakes.data)\n",
    "                gradient_penalty.backward()\n",
    "\n",
    "                d_loss = d_loss_fake - d_loss_real + gradient_penalty\n",
    "                Wasserstein_D = d_loss_real - d_loss_fake\n",
    "                self.d_optimizer.step()\n",
    "                \n",
    "            # 固定判别器，训练生成器\n",
    "            for p in self.D.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.G.zero_grad()\n",
    "            inputs, reals = next(self.data)\n",
    "            fakes = self.G(inputs)\n",
    "            g_loss = self.D(fakes).mean()\n",
    "            g_cost = -g_loss\n",
    "            \n",
    "            # L1 损失\n",
    "            l1_loss = self.loss_function(fakes, reals).mean()\n",
    "            \n",
    "            l = 1000 * l1_loss + g_cost\n",
    "            l.backward()\n",
    "            \n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            print(f'D loss: {d_loss.item()}, G loss: {g_loss.item()}, l1 loss: {100 * l1_loss.item()}, Wasserstein_D: {Wasserstein_D.item()}, cost time: {time.time() - begin}')\n",
    "            self.save_model()\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                torch.save(self.G.state_dict(), f'{model_dir}/{model_name}-generator-{epoch + start_point + 1}.pth')\n",
    "            if (epoch + 1) % 500 == 0:\n",
    "                torch.save(self.D.state_dict(), f'{model_dir}/{model_name}-discriminator-{epoch + start_point + 1}.pth')\n",
    "\n",
    "    def get_infinite_batches(self):\n",
    "        while True:\n",
    "            for i, (inputs, reals) in enumerate(self.data_loader):\n",
    "                inputs = inputs.to(self.device)\n",
    "                reals = reals.to(self.device)\n",
    "                yield inputs, reals\n",
    "\n",
    "    def calculate_gradient_penalty(self, real_images, fake_images):\n",
    "        eta = torch.FloatTensor(self.batch_size, 1, 1, 1).uniform_(0, 1)\n",
    "        eta = eta.expand(self.batch_size, real_images.size(1), real_images.size(2), real_images.size(3))\n",
    "        eta = eta.to(self.device)\n",
    "\n",
    "        interpolated = eta * real_images + ((1 - eta) * fake_images)\n",
    "        interpolated = interpolated.to(self.device)\n",
    "\n",
    "        # define it to calculate gradient\n",
    "        interpolated = Variable(interpolated, requires_grad=True)\n",
    "\n",
    "        # calculate probability of interpolated examples\n",
    "        prob_interpolated = self.D(interpolated)\n",
    "\n",
    "        # calculate gradients of probabilities with respect to examples\n",
    "        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                                  grad_outputs=torch.ones(\n",
    "                                      prob_interpolated.size()).cuda(0),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.lambda_term\n",
    "        return grad_penalty\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.G.state_dict(), generator_model_name)\n",
    "        torch.save(self.D.state_dict(), discriminator_model_name)\n",
    "        print(f'Models save to {generator_model_name} & {discriminator_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd931e1-a3ed-4541-8e4f-ff836f4980f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WGAN()\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
